\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{abbeel2007application}
\citation{abbeel2008apprenticeship}
\citation{van2010superhuman}
\citation{laskeyshiv}
\citation{laskeyrobot}
\citation{ross2010efficient}
\citation{ross2010reduction}
\citation{laskeyrobot}
\citation{laskeyshiv}
\citation{he2012imitation}
\citation{ross2010efficient}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  A) An illustration of the expert's policy, $\pi _{\theta ^*}$ being contained in the robot's policy class $\Theta $. We argue that in the case on the right, passive LfD can be advantageous compare to adaptive. B) Samples taken from a robot trying to reach the green goal state. The adaptive approach(Teal) causes the robot to visit states the passive (Orange) does not need to. C) A person from our pilot study using an Xbox Controller to provide a demonstration to the robot on how to singulate an object from the pile. D) In the pilot study, passive (left) always had visit the states the expert did, but adaptive (right) lead to states where the expert had to provide more complex commands such as teaching the robot to go backwards.\relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaserl}{{1}{1}{\footnotesize A) An illustration of the expert's policy, $\pi _{\theta ^*}$ being contained in the robot's policy class $\Theta $. We argue that in the case on the right, passive LfD can be advantageous compare to adaptive. B) Samples taken from a robot trying to reach the green goal state. The adaptive approach(Teal) causes the robot to visit states the passive (Orange) does not need to. C) A person from our pilot study using an Xbox Controller to provide a demonstration to the robot on how to singulate an object from the pile. D) In the pilot study, passive (left) always had visit the states the expert did, but adaptive (right) lead to states where the expert had to provide more complex commands such as teaching the robot to go backwards.\relax }{figure.caption.1}{}}
\citation{pomerleau1989alvinn}
\citation{NVIDEA}
\citation{ross2010efficient}
\citation{ross2013learning}
\citation{laskeyshiv}
\citation{laskeyrobot}
\citation{kim2013maximum}
\citation{duvallet2013imitation}
\citation{he2012imitation}
\citation{kim2013maximum}
\citation{laskeyshiv}
\citation{laskeyrobot}
\citation{ross2010reduction}
\citation{shalev2011online}
\citation{ross2010reduction}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Problem Statement and Background}{2}{section.3}}
\newlabel{sec:PS}{{III}{2}{Problem Statement and Background}{section.3}{}}
\citation{levine2015end}
\citation{ross2010reduction}
\citation{ross2010efficient}
\citation{ross2010reduction}
\citation{ross2010reduction}
\citation{scholkopf2002learning}
\citation{NIPS2014_5421}
\citation{ross2010reduction}
\newlabel{eq:m_likeli_obj}{{1}{3}{Problem Statement and Background}{equation.3.1}{}}
\newlabel{eq:main_obj}{{2}{3}{Problem Statement and Background}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {III-.1}Step 1}{3}{subsubsection.3.0.1}}
\newlabel{eq:super_objj}{{3}{3}{Step 1}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {III-.2}Step 2}{3}{subsubsection.3.0.2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Theoretical Analysis}{3}{section.4}}
\citation{vapnik1992principles}
\citation{ross2010reduction}
\citation{ross2010reduction}
\citation{shalev2011online}
\citation{kakade2009generalization}
\citation{ross2010efficient}
\citation{levine2015end}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  A binary decision tree, where a robot is being taught by a expert to descend down and achieve maximum cumulative reward, which is shown via the numbers on each branch. Each node has the action the expert would select, either left or right, $\delimiter "4266308 L, R \delimiter "5267309 $. The optimal policy, colored in green, is to select left, $L$, at each node.\relax }}{4}{figure.caption.2}}
\newlabel{fig:c_ex}{{2}{4}{\footnotesize A binary decision tree, where a robot is being taught by a expert to descend down and achieve maximum cumulative reward, which is shown via the numbers on each branch. Each node has the action the expert would select, either left or right, $\lbrace L, R \rbrace $. The optimal policy, colored in green, is to select left, $L$, at each node.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Algorithm Convergence}{4}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Bound on Error for Passive Lfd}{4}{subsection.4.2}}
\citation{anthony2009neural}
\citation{vapnik2013nature}
\citation{vapnik2013nature}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-C}Finite Sample Analysis}{5}{subsection.4.3}}
\newlabel{thm:sup}{{4.3}{5}{Finite Sample Analysis}{theorem.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiments}{5}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-A}Varying Function Class}{5}{subsection.5.1}}
\newlabel{sec:gdw}{{V-A}{5}{Varying Function Class}{subsection.5.1}{}}
\citation{ross2010efficient}
\citation{ross2010reduction}
\citation{ross2013learning}
\citation{ross2010efficient}
\citation{ross2010reduction}
\citation{laskeyrobot}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Shown above is the normalized performance with respect to the expected expert, where $1.0$ indicates matching the expected expert's reward. The plots are averaged over 100 randomly generated 2D gridworld environments, where the robot is taught to avoid penalty states and reach a goal state. Condition A examines when the robot's policy class is not able to learn the expert's, which results in adaptivity leading to better performance. Condition D examines a larger robot policy class that contains the expected expert,and demonstrates negligible difference between adaptive and passive LfD. to represent the expert. This leads to similar performance as Condition B, but requires more data. Finally, Condition C examines when the expert has noise added to the controls labels, this leads to more data being needed to converge to the expected expert, but the difference between passive and adaptive is still negligible. \relax }}{6}{figure.caption.3}}
\newlabel{fig:var}{{3}{6}{\footnotesize Shown above is the normalized performance with respect to the expected expert, where $1.0$ indicates matching the expected expert's reward. The plots are averaged over 100 randomly generated 2D gridworld environments, where the robot is taught to avoid penalty states and reach a goal state. Condition A examines when the robot's policy class is not able to learn the expert's, which results in adaptivity leading to better performance. Condition D examines a larger robot policy class that contains the expected expert,and demonstrates negligible difference between adaptive and passive LfD. to represent the expert. This leads to similar performance as Condition B, but requires more data. Finally, Condition C examines when the expert has noise added to the controls labels, this leads to more data being needed to converge to the expected expert, but the difference between passive and adaptive is still negligible. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-B}Algorithmic Convergence }{6}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-C}Human Study for Planar Singulation}{6}{subsection.5.3}}
\citation{laskeyshiv}
\citation{laskeyrobot}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Left: A 2D workspace where a point mass robot is taught to go to the green circle starting from the blue circle. The world is divided into to two quadrants A and B, in B the controls are inverted in the dynamics thus resulting in $x=y$ and $y=x$. The expert is the infinite horizion LQG computed policy, which results in two different linear matrices in region A and B. Right: Illustrates how having a linear robot policy class can cause DAgger to fail to converge due to it collecting data from region B. \relax }}{7}{figure.caption.4}}
\newlabel{fig:p_mass}{{4}{7}{\footnotesize Left: A 2D workspace where a point mass robot is taught to go to the green circle starting from the blue circle. The world is divided into to two quadrants A and B, in B the controls are inverted in the dynamics thus resulting in $x=y$ and $y=x$. The expert is the infinite horizion LQG computed policy, which results in two different linear matrices in region A and B. Right: Illustrates how having a linear robot policy class can cause DAgger to fail to converge due to it collecting data from region B. \relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Shown above is an example of an initial state (left) that the robot is presented with. It can vary in placement of the object in the pile, translation and rotation. A human is asked to singulate the object, which is to have the robot learn to push one object from the pile (right). \relax }}{7}{figure.caption.5}}
\newlabel{fig:izzy_rw}{{5}{7}{\footnotesize Shown above is an example of an initial state (left) that the robot is presented with. It can vary in placement of the object in the pile, translation and rotation. A human is asked to singulate the object, which is to have the robot learn to push one object from the pile (right). \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion and Future Work}{7}{section.6}}
\bibstyle{IEEEtranS}
\bibdata{references}
\bibcite{abbeel2007application}{1}
\bibcite{abbeel2008apprenticeship}{2}
\bibcite{anthony2009neural}{3}
\bibcite{duvallet2013imitation}{4}
\bibcite{NIPS2014_5421}{5}
\bibcite{kakade2009generalization}{6}
\bibcite{kim2013maximum}{7}
\bibcite{laskeyrobot}{8}
\bibcite{laskeyshiv}{9}
\bibcite{levine2015end}{10}
\bibcite{pomerleau1989alvinn}{11}
\bibcite{ross2010efficient}{12}
\bibcite{ross2010reduction}{13}
\bibcite{ross2013learning}{14}
\bibcite{scholkopf2002learning}{15}
\bibcite{shalev2011online}{16}
\bibcite{van2010superhuman}{17}
\bibcite{vapnik1992principles}{18}
\bibcite{vapnik2013nature}{19}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Shown is averaged success at the singulation task over the 10 human subjects. Each policy is evaulated 30 times on the a held out set of test configurations. The first 20 rollouts are from the expert rolling out there policy and the next 40 are collected via retro-active feedback for adaptive and tele-operated demonstrations for passive. Passive LfD shows a 20$\%$ improvement in success at the end. \relax }}{8}{figure.caption.6}}
\newlabel{fig:izzy_rw}{{6}{8}{\footnotesize Shown is averaged success at the singulation task over the 10 human subjects. Each policy is evaulated 30 times on the a held out set of test configurations. The first 20 rollouts are from the expert rolling out there policy and the next 40 are collected via retro-active feedback for adaptive and tele-operated demonstrations for passive. Passive LfD shows a 20$\%$ improvement in success at the end. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{References}{8}{section*.7}}
\@writefile{toc}{\contentsline {section}{Appendix}{8}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Notation and Problem Statement}{8}{subsection.Appendix.A.1}}
\newlabel{sde}{{14}{8}{Notation and Problem Statement}{equation.Appendix.A.14}{}}
\newlabel{obj}{{15}{9}{Notation and Problem Statement}{equation.Appendix.A.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Empirical Risk Minimizer (Passive LfD)}{9}{subsection.Appendix.A.2}}
\newlabel{erm}{{16}{9}{Empirical Risk Minimizer (Passive LfD)}{equation.Appendix.A.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Follow-The-Leader (DAgger)}{9}{subsection.Appendix.A.3}}
