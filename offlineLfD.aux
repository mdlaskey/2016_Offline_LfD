\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{abbeel2007application}
\citation{abbeel2008apprenticeship}
\citation{van2010superhuman}
\citation{laskeyshiv}
\citation{laskeyrobot}
\citation{pomerleau1989alvinn}
\citation{ross2010efficient}
\citation{ross2010reduction}
\citation{laskeyrobot}
\citation{laskeyshiv}
\citation{he2012imitation}
\citation{vapnik1992principles}
\citation{ross2010efficient}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  a) A person from our pilot study using an Xbox Controller to provide a demonstration to the robot on how to singulate (i.e. seperate) an object from the pile. b) [Changing this part] c) Two states that a human had to provide retro-active feedback on from the pilot study. Left: a state that the robot would have seen under HC sampling. Right: a state the robot would not have seen under HC sampling. Correct feedback in this state would be to tell the robot to recover by going backwards and to the side. [STILL WORKING ON FIGURE PLAYING WITH LIGHTING AND DISPLAY]\relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaserl}{{1}{1}{\footnotesize a) A person from our pilot study using an Xbox Controller to provide a demonstration to the robot on how to singulate (i.e. seperate) an object from the pile. b) [Changing this part] c) Two states that a human had to provide retro-active feedback on from the pilot study. Left: a state that the robot would have seen under HC sampling. Right: a state the robot would not have seen under HC sampling. Correct feedback in this state would be to tell the robot to recover by going backwards and to the side. [STILL WORKING ON FIGURE PLAYING WITH LIGHTING AND DISPLAY]\relax }{figure.caption.1}{}}
\citation{pomerleau1989alvinn}
\citation{NVIDEA}
\citation{ross2010efficient}
\citation{ross2013learning}
\citation{laskeyshiv}
\citation{laskeyrobot}
\citation{kim2013maximum}
\citation{duvallet2013imitation}
\citation{he2012imitation}
\citation{kim2013maximum}
\citation{laskeyshiv}
\citation{laskeyrobot}
\citation{ross2010reduction}
\citation{shalev2011online}
\citation{ross2010reduction}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Problem Statement and Background}{2}{section.3}}
\newlabel{sec:PS}{{III}{2}{Problem Statement and Background}{section.3}{}}
\citation{mahler2014learning}
\citation{ross2010reduction}
\citation{ross2010efficient}
\citation{ross2010reduction}
\citation{ross2010reduction}
\citation{laskeyshiv}
\citation{he2012imitation}
\citation{ross2010reduction}
\citation{scholkopf2002learning}
\citation{NIPS2014_5421}
\citation{ross2010reduction}
\newlabel{eq:m_likeli_obj}{{1}{3}{Problem Statement and Background}{equation.3.1}{}}
\newlabel{eq:main_obj}{{2}{3}{Problem Statement and Background}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {III-.1}Step 1}{3}{subsubsection.3.0.1}}
\newlabel{eq:super_objj}{{3}{3}{Step 1}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {III-.2}Step 2}{3}{subsubsection.3.0.2}}
\citation{vapnik1992principles}
\citation{ross2010reduction}
\citation{ross2010reduction}
\citation{shalev2011online}
\citation{ross2010efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  A binary decision tree, where a robot is being taught by a supervisor to descend down and achieve maximum cumulative reward, which is shown via the numbers on each branch. Each node has the action the supervisor would select, either left or right, $\delimiter "4266308 L, R \delimiter "5267309 $. The HC method converges to the Orange path, which is optimal. While the RC method converges to the Teal path, because it tries to learn on examples from that side of the tree.\relax }}{4}{figure.caption.2}}
\newlabel{fig:c_ex}{{2}{4}{\footnotesize A binary decision tree, where a robot is being taught by a supervisor to descend down and achieve maximum cumulative reward, which is shown via the numbers on each branch. Each node has the action the supervisor would select, either left or right, $\lbrace L, R \rbrace $. The HC method converges to the Orange path, which is optimal. While the RC method converges to the Teal path, because it tries to learn on examples from that side of the tree.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Theoretical Analysis}{4}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Algorithm Convergence}{4}{subsection.4.1}}
\citation{mahler2014learning}
\citation{ross2010reduction}
\citation{verdu2014total}
\citation{ross2010reduction}
\citation{anthony2009neural}
\citation{bartlett2002rademacher}
\citation{kakade2009complexity}
\citation{vapnik2013nature}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Bound on Error for HC Lfd}{5}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiments}{5}{section.5}}
\citation{rossreduction2010}
\citation{ross2010efficient}
\citation{ross2010reduction}
\citation{ross2013learning}
\citation{ross2010efficient}
\citation{ross2010reduction}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Shown above is the normalized performance with respect to the expected supervisor, where $1.0$ indicates matching the optimal supervisor's performance. The plots are averaged over 100 randomly generated 2D gridworld environments, where the robot is taught to avoid penalty states and reach a goal state. Both RC and HC are given the same number of samples. A) Examines when the robot's policy class has low expressiveness (i.e. Linear SVM), which results in RC leading to better performance. Condition B examines a more expressive robot policy class (i.e. Decision Trees) that contains the expected supervisor,and demonstrates negligible difference between HC and RC. to represent the supervisor. Finally, Condition C examines when the supervisor has noise added to the controls labels, this leads to more data being needed to converge to the expected supervisor. HC and RC perform the same in this situation. \relax }}{6}{figure.caption.3}}
\newlabel{fig:var}{{3}{6}{\footnotesize Shown above is the normalized performance with respect to the expected supervisor, where $1.0$ indicates matching the optimal supervisor's performance. The plots are averaged over 100 randomly generated 2D gridworld environments, where the robot is taught to avoid penalty states and reach a goal state. Both RC and HC are given the same number of samples. A) Examines when the robot's policy class has low expressiveness (i.e. Linear SVM), which results in RC leading to better performance. Condition B examines a more expressive robot policy class (i.e. Decision Trees) that contains the expected supervisor,and demonstrates negligible difference between HC and RC. to represent the supervisor. Finally, Condition C examines when the supervisor has noise added to the controls labels, this leads to more data being needed to converge to the expected supervisor. HC and RC perform the same in this situation. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-A}Varying Function Class}{6}{subsection.5.1}}
\newlabel{sec:gdw}{{V-A}{6}{Varying Function Class}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-B}Algorithmic Convergence }{6}{subsection.5.2}}
\citation{laskeyrobot}
\citation{laskeyshiv}
\citation{laskeyrobot}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Left: A 2D workspace where a point mass robot is taught to go to the green circle starting from the blue circle. The world is divided into to two quadrants 1 and 2, in 2 the point mass has four times as much mass. The supervisor is computed via infinite horizon LQG for each region, which results in two different linear matrices in region 1 and 2. Right: Illustrates how having a linear robot policy class can cause RC LfD to fail to converge due to it collecting data from region 2, however HC converges to the true supervisor performance. \relax }}{7}{figure.caption.4}}
\newlabel{fig:p_mass}{{4}{7}{\footnotesize Left: A 2D workspace where a point mass robot is taught to go to the green circle starting from the blue circle. The world is divided into to two quadrants 1 and 2, in 2 the point mass has four times as much mass. The supervisor is computed via infinite horizon LQG for each region, which results in two different linear matrices in region 1 and 2. Right: Illustrates how having a linear robot policy class can cause RC LfD to fail to converge due to it collecting data from region 2, however HC converges to the true supervisor performance. \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-C}Human Study for Planar Singulation}{7}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Shown above is an example of an initial state (left) that the robot is presented with. It can vary in placement of the object in the pile, translation and rotation. A human is asked to singulate the object, which is to have the robot learn to push one object from the pile (right). A successful singulation means at least one object has its center located 10 cm or more from all other object centers. \relax }}{7}{figure.caption.5}}
\newlabel{fig:izzy_sing}{{5}{7}{\footnotesize Shown above is an example of an initial state (left) that the robot is presented with. It can vary in placement of the object in the pile, translation and rotation. A human is asked to singulate the object, which is to have the robot learn to push one object from the pile (right). A successful singulation means at least one object has its center located 10 cm or more from all other object centers. \relax }{figure.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Shown above is the average surrogate loss on a held out set of 10 demonstrations from the the total 60 demonstrations collected for each 10 demonstrators. The confidence intervals are standard error on the mean, which indicate RC LfD obtains a statistically significant higher surrogate loss in both degrees of freedom, forward and rotation. \relax }}{8}{table.caption.6}}
\newlabel{tab:opt-p-comparison}{{I}{8}{\footnotesize Shown above is the average surrogate loss on a held out set of 10 demonstrations from the the total 60 demonstrations collected for each 10 demonstrators. The confidence intervals are standard error on the mean, which indicate RC LfD obtains a statistically significant higher surrogate loss in both degrees of freedom, forward and rotation. \relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Shown above are samples collected during the human study on the same initial state, each point on the line represents where the tip of the gripper is. Left: Is the the 10 demonstrations given from human tele-operation. Note they all move to the object pile and try to push the "banana" shape to move the small shape. Right: the trajectories from RC sampling under the current policy trained. The RC samples visit a much larger area of the workspace, where the robot must be taught more complex recovery behavior. \relax }}{8}{figure.caption.7}}
\newlabel{fig:izzy_traj}{{6}{8}{\footnotesize Shown above are samples collected during the human study on the same initial state, each point on the line represents where the tip of the gripper is. Left: Is the the 10 demonstrations given from human tele-operation. Note they all move to the object pile and try to push the "banana" shape to move the small shape. Right: the trajectories from RC sampling under the current policy trained. The RC samples visit a much larger area of the workspace, where the robot must be taught more complex recovery behavior. \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Shown is averaged success at the singulation task over the 10 human subjects. Each policy is evaulated 30 times on the a held out set of test configurations. The first 20 rollouts are from the supervisor rolling out there policy and the next 40 are collected via retro-active feedback for adaptive and tele-operated demonstrations for passive. HC LfD shows a 20$\%$ improvement in success at the end. \relax }}{8}{figure.caption.8}}
\newlabel{fig:izzy_rw}{{7}{8}{\footnotesize Shown is averaged success at the singulation task over the 10 human subjects. Each policy is evaulated 30 times on the a held out set of test configurations. The first 20 rollouts are from the supervisor rolling out there policy and the next 40 are collected via retro-active feedback for adaptive and tele-operated demonstrations for passive. HC LfD shows a 20$\%$ improvement in success at the end. \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion and Future Work}{8}{section.6}}
\bibstyle{IEEEtranS}
\bibdata{references}
\bibcite{abbeel2007application}{1}
\bibcite{abbeel2008apprenticeship}{2}
\bibcite{anthony2009neural}{3}
\bibcite{bartlett2002rademacher}{4}
\bibcite{duvallet2013imitation}{5}
\bibcite{NIPS2014_5421}{6}
\bibcite{kakade2009complexity}{7}
\bibcite{kim2013maximum}{8}
\bibcite{laskeyrobot}{9}
\bibcite{laskeyshiv}{10}
\bibcite{levine2015end}{11}
\bibcite{pomerleau1989alvinn}{12}
\bibcite{ross2010efficient}{13}
\bibcite{ross2010reduction}{14}
\bibcite{ross2013learning}{15}
\bibcite{scholkopf2002learning}{16}
\bibcite{shalev2011online}{17}
\bibcite{tewari13learning}{18}
\bibcite{van2010superhuman}{19}
\bibcite{vapnik1992principles}{20}
\bibcite{vapnik2013nature}{21}
\bibcite{verdu2014total}{22}
\@writefile{toc}{\contentsline {section}{References}{9}{section*.9}}
