\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{pomerleau1989alvinn}
\citation{ross2010reduction}
\citation{ross2010efficient}
\citation{ross2010reduction}
\citation{ross2013learning}
\citation{vapnik2013nature}
\citation{buhlmann2003boosting}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  An illustration of the of realizable. Denote a policy class $\Theta $ and a supervisor policy $\mathaccentV {tilde}07E{\pi }$, the supervisor's policy is realizable if it is contained in $\Theta $. We also illustrate Boosting, which can be thought of as adding to the function class to achieve realizability.\relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaserl}{{1}{1}{\footnotesize An illustration of the of realizable. Denote a policy class $\Theta $ and a supervisor policy $\tilde {\pi }$, the supervisor's policy is realizable if it is contained in $\Theta $. We also illustrate Boosting, which can be thought of as adding to the function class to achieve realizability.\relax }{figure.caption.1}{}}
\citation{laskeyshiv}
\citation{laskeyrobot}
\citation{kim2013maximum}
\citation{duvallet2013imitation}
\citation{ross2010efficient}
\citation{kakade2003sample}
\citation{chernova2009interactive}
\citation{judah2011active}
\citation{grollman2007dogged}
\citation{kim2013maximum}
\citation{laskeyshiv}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Problem Statement}{2}{section.3}}
\newlabel{sec:PS}{{III}{2}{Problem Statement}{section.3}{}}
\citation{levine2015end}
\citation{ross2010reduction}
\citation{levine2015end}
\citation{mason1999boosting}
\citation{anthony2009neural}
\citation{vapnik2013nature}
\newlabel{eq:m_likeli_obj}{{1}{3}{Problem Statement}{equation.3.1}{}}
\newlabel{eq:main_obj}{{2}{3}{Problem Statement}{equation.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Passive LfD}{3}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Passive LfD}{3}{subsection.4.1}}
\newlabel{thm:sup}{{4.1}{3}{Passive LfD}{theorem.4.1}{}}
\citation{ross2010reduction}
\citation{scholkopf2002learning}
\citation{NIPS2014_5421}
\citation{ross2010reduction}
\citation{ross2010reduction}
\citation{shalev2011online}
\@writefile{toc}{\contentsline {section}{\numberline {V}DAgger}{4}{section.5}}
\newlabel{sec:DAgger}{{V}{4}{DAgger}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-A}Algorithm}{4}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {V-A.1}Step 1}{4}{subsubsection.5.1.1}}
\newlabel{eq:super_objj}{{3}{4}{Step 1}{equation.5.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {V-A.2}Step 2}{4}{subsubsection.5.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  A binary decision tree, where a robot is being taught by a supervisor to descend down and achieve maximum cumulative reward, which is shown via the numbers on each branch. Each node has the action the supervisor would select, either left or right, $\delimiter "4266308 L, R \delimiter "5267309 $. The optimal policy, colored in green, is to select left, $L$, at each node.\relax }}{4}{figure.caption.2}}
\newlabel{fig:c_ex}{{2}{4}{\footnotesize A binary decision tree, where a robot is being taught by a supervisor to descend down and achieve maximum cumulative reward, which is shown via the numbers on each branch. Each node has the action the supervisor would select, either left or right, $\lbrace L, R \rbrace $. The optimal policy, colored in green, is to select left, $L$, at each node.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-B}Analysis of DAgger}{4}{subsection.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Experiments}{4}{section.6}}
\citation{scikit-learn}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Shown above is the expected reward obtained for Value Iteration, which is the optimal supervisor, Supervise learning using Adaboost, DAgger and Supervise learning. All results are averaged over 1500 trials. In A) we demonstrate the four approaches for a given dynamics and initial state distributions, Adaboost converges fastest to the supervisor. In B), we double the noise in the dynamics, which causes the robot to cover a lot more states. Adaboost is still able to achieve faster convergence, which suggests DAgger does not help in producing a covering of states. Finally, in C) we increase the initial state variance by 5x, to try and see if over-fitting to a small sample size is possible. Results suggests that the effects of over-fitting persists in both DAgger and Adaboost.\relax }}{5}{figure.caption.3}}
\newlabel{fig:var}{{3}{5}{\footnotesize Shown above is the expected reward obtained for Value Iteration, which is the optimal supervisor, Supervise learning using Adaboost, DAgger and Supervise learning. All results are averaged over 1500 trials. In A) we demonstrate the four approaches for a given dynamics and initial state distributions, Adaboost converges fastest to the supervisor. In B), we double the noise in the dynamics, which causes the robot to cover a lot more states. Adaboost is still able to achieve faster convergence, which suggests DAgger does not help in producing a covering of states. Finally, in C) we increase the initial state variance by 5x, to try and see if over-fitting to a small sample size is possible. Results suggests that the effects of over-fitting persists in both DAgger and Adaboost.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-A}GridWorld}{5}{subsection.6.1}}
\newlabel{sec:gdw}{{VI-A}{5}{GridWorld}{subsection.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-B}Point Mass Control}{5}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-C}Planar Singulation}{5}{subsection.6.3}}
\citation{laskeyrobot}
\citation{laskeyshiv}
\bibstyle{IEEEtranS}
\bibdata{references}
\bibcite{abbeel2007application}{1}
\bibcite{abbeel2008apprenticeship}{2}
\bibcite{anthony2009neural}{3}
\bibcite{chernova2009interactive}{4}
\bibcite{duvallet2013imitation}{5}
\bibcite{grollman2007dogged}{6}
\bibcite{NIPS2014_5421}{7}
\bibcite{judah2011active}{8}
\bibcite{kakade2003sample}{9}
\bibcite{kim2013maximum}{10}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Shown is Boosted passive compared against Boosted DAgger in a 3D gridworld domain with each iteration corresponding to a demonstration given via the supervisor controlling the system or retro-actively, respectively. All results are averaged over 1000 trails. With low initial state variance the performance is similar, however as the initial state variance is increase DAgger queries the supervisor for less informative demonstrations. Thus, resulting in slower convergence. \relax }}{6}{figure.caption.4}}
\newlabel{fig:p_mass}{{4}{6}{\footnotesize Shown is Boosted passive compared against Boosted DAgger in a 3D gridworld domain with each iteration corresponding to a demonstration given via the supervisor controlling the system or retro-actively, respectively. All results are averaged over 1000 trails. With low initial state variance the performance is similar, however as the initial state variance is increase DAgger queries the supervisor for less informative demonstrations. Thus, resulting in slower convergence. \relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Shown is Boosted passive compared against Boosted DAgger in a 3D gridworld domain with each iteration corresponding to a demonstration given via the supervisor controlling the system or retro-actively, respectively. All results are averaged over 1000 trails. With low initial state variance the performance is similar, however as the initial state variance is increase DAgger queries the supervisor for less informative demonstrations. Thus, resulting in slower convergence. \relax }}{6}{figure.caption.5}}
\newlabel{fig:izzy_rw}{{5}{6}{\footnotesize Shown is Boosted passive compared against Boosted DAgger in a 3D gridworld domain with each iteration corresponding to a demonstration given via the supervisor controlling the system or retro-actively, respectively. All results are averaged over 1000 trails. With low initial state variance the performance is similar, however as the initial state variance is increase DAgger queries the supervisor for less informative demonstrations. Thus, resulting in slower convergence. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Conclusion}{6}{section.7}}
\@writefile{toc}{\contentsline {section}{References}{6}{section*.6}}
\bibcite{laskeyrobot}{11}
\bibcite{laskeyshiv}{12}
\bibcite{levine2015end}{13}
\bibcite{mason1999boosting}{14}
\bibcite{scikit-learn}{15}
\bibcite{pomerleau1989alvinn}{16}
\bibcite{ross2010efficient}{17}
\bibcite{ross2010reduction}{18}
\bibcite{ross2013learning}{19}
\bibcite{scholkopf2002learning}{20}
\bibcite{settles2008analysis}{21}
\bibcite{van2010superhuman}{22}
\bibcite{vapnik2013nature}{23}
