%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

%\documentclass[journal,transmag]{IEEEtran}% Comment this line out if you need a4paper

\documentclass[10pt, conference]{ieeeconf}      % Use this line for a4 paper


\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

%\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{url}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\def\lc{\left\lfloor}   
\def\rc{\right\rfloor}

\usepackage{amsmath,amssymb}

\usepackage{tabularx}
\usepackage{tikz,hyperref,graphicx,units}
\usepackage{subfigure}
\usepackage{benktools}
\usepackage{bbm}
\renewcommand{\baselinestretch}{.5}

\usepackage{caption}
\usepackage{epstopdf}
\renewcommand{\captionfont}{\footnotesize}
\usepackage{sidecap,wrapfig}
\usepackage[ruled,vlined]{algorithm2e}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\abs}[1]{\lvert#1\rvert} 
\newcommand{\norm}[1]{\lVert#1\rVert}
%\newcommand{\suchthat}{\mid}
\newcommand{\suchthat}{\ \big|\ }
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\mX}{\mathcal{X}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\mG}{\mathcal{G}}
\newcommand{\mN}{\mathcal{N}}
\newcommand{\mW}{\mathcal{W}}
\newcommand{\mF}{\mathcal{F}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bbZ}{\mathbb{Z}}
\newcommand{\mR}{\mathcal{R}}


\newcommand{\bfc}{W}
\newcommand{\Qinf}{Q_{\infty}}
\newcommand{\st}[1]{_\text{#1}}
\newcommand{\rres}{r\st{res}}
\newcommand{\pos}[1]{(#1)^+}
\newcommand{\depth}{\operatorname{depth}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\convhull}{\operatorname{ConvexHull}}
\newcommand{\minksum}{\operatorname{MinkowskiSum}}

\newcommand{\specialcell}[2][c]{ \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand{\acro}{SHIV}
\newcommand{\ns}{HC LfD }
\newcommand{\nc}{RC LfD }
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcolumntype{L}[1]{>{\RaggedRight\hspace{0pt}}p{#1}}
\newcolumntype{R}[1]{>{\RaggedLeft\hspace{0pt}}p{#1}}

%\newtheorem{lemma}{Lemma}[section]
%\newtheorem{theorem}{Theorem}[section]
\newtheorem{defn}{Definition}[section]

\newboolean{include-notes}
\setboolean{include-notes}{true}
\newcommand{\adnote}[1]{\ifthenelse{ \boolean{include-notes}}%
 {\textcolor{blue}{\textbf{AD: #1}}}{}}
 
 \newcommand{\sknote}[1]{\ifthenelse{ \boolean{include-notes}}%
 {\textcolor{blue}{\textbf{SK: #1}}}{}}
 
  \newcommand{\mlnote}[1]{\ifthenelse{ \boolean{include-notes}}%
 {\textcolor{purple}{\textbf{ML: #1}}}{}}
 
 \newcommand{\jmnote}[1]{\ifthenelse{ \boolean{include-notes}}%
 {\textcolor{orange}{\textbf{JM: #1}}}{}}

\renewcommand{\baselinestretch}{.95}
\usepackage{times}
\usepackage{microtype}
%\title{Iterative Imitation Learning with Reduced Human Supervision [v11]}
%\title{SHIV:  Reducing Human Supervision for Robot adaptive Learning [v11]}

\title{Comparing Human-Centric and Robot-Centric \\
Sample Efficiency for Robot Deep Learning from Demonstrations}



\author{Michael Laskey, Caleb Chuck, Jonathan Lee, Jeffrey Mahler,\\ Sanjay Krishnan, Kevin Jamieson, Anca Dragan, Ken Goldberg}
\begin{document}


\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
We characterize two approaches to acquiring samples in supervised learning of robot control policies from demonstration. The first is the standard supervised learning approach, where a human supervisor demonstrates the task by providing policy roll-outs consisting of state-action samples. We refer to this as “Human Centric” (HC) sampling. We contrast it to "Robot-Centric" (RC) sampling, where a human supervisor iteratively observes the robot's current learned policy and provides new action labels to correct errors.  
%move to intro RC sampling is motivated by the fact that when the robot can not match the supervisor’s policy well, errors will cause it to veer off to states with limited data, in which it will perform poorly. RC sampling collects data on such states and has been used in algorithms like DAgger to improve performance. 
RC sampling has been used in algorithms like DAgger to collect data on states that the robot will reach when it can not match the supervisor's policy well, which is advantageous.  
Our insight is that this RC sampling advantage does come at a cost in practice. First, it is tedious and prone to mislabeling for human supervisors because it requires providing corrective actions without observing their outcome on the robot. Second, it requires learning a more complex policy than the standard supervisor policy, involving corrections from failure states. This can, in some cases, actually increase sample complexity.  Motivated by this insight, we explore the value of the extra overhead of RC for emerging classes of highly-expressive learning models, such as deep decision trees and neural networks, which can more accurately learn the supervisor policy compared to standard learners.  We provide a new analysis on HC sampling performance for the case of expressive learners, and also compare the performance of HC and RC sampling on a set of robot tasks.  We confirmed that RC is valuable for standard learning models such as SVMs, but were surprised by results suggesting that HC sampling may be preferable for highly-expressive learning models like deep learning.

 \end{abstract}






\section{Introduction} 

We focus on Learning from Demonstration (LfD), in which a robot learns to perform a task via examples from a human supervisor \cite{abbeel2008apprenticeship,argall2009surveyross,2010reduction,laskeyshiv}.  LfD can be tedious, requiring many demonstrations for each task. Our ultimate goal is to improve the data efficiency in LfD, making it quick and easy to teach the robot new tasks. We take a step in this direction by analyzing two different approaches for acquiring supervisor data. 

In the supervised learning approach to LfD, the robot learns a policy a mapping from the state space to controls  via regression or classification. The standard approach to LfD is to let the supervisor provide multiple demonstrations of performing the task, which induces a set of state-control examples for the states the supervisor is likely to visit~\cite{pomerleau1989alvinn}. We refer to this as \emph{Human-Centric} (HC) sampling, because the human controls what states the robot will observe. Because of error in learning, the robot's policy will not perfectly match the supervisor, and will end up visiting new states when performing the task. An alternative approach to HC sampling is to have the supervisor retro-actively provide corrections to the robot \cite{ross2010efficient,ross2010reduction,laskeyrobot,laskeyshiv,he2012imitation}, i.e. iteratively re-label the states that the robot visits with its current learned policy.
%feedback (or the correct control signal) to the robot on the states that it visits, this refers to DAgger, SHIV and variants~\cite{ross2010efficient,ross2010reduction,laskeyrobot,laskeyshiv,he2012imitation}.
These attempts to provide examples on states the robot is more likely to see.  We refer to this as Robot-Centric (RC) sampling, because the robot dictates the choice of states that the supervisor labels.

\begin{figure}
\center
\includegraphics[width=0.5\textwidth]{f_figs/teaser.eps}
\caption{
    \footnotesize
a) Prior work has shown for less expressive policies that RC methods outperform HC. However, if the policy is expressive enough to capture the supervisor then with infinite data HC is optimal. We examine how HC performs against RC is the high expressive and finite sample case.  b) An example of a human trying to match their teleoperated path (black) with retroactive feedback (teal). When the controls do match the teal line should be tangent to the path.  c) Samples collected during the human study on the same initial state. Left:  10 demonstrations given from human teleoperation. Note: they all move to the object pile and singulate the small object. Right: The trajectories from RC sampling under the current policy trained. The RC samples force the robot to areas where it needs to try and learn more complex behavior, like going backwards. }
\vspace*{-20pt}2
\label{fig:teaser}
\end{figure}


Our insight is that while RC has significant advantages, it comes at a cost in practice. One cost is the challenge posed to a \emph{human} supervisor.  RC methods have a human provide retro-active feedback to the robot without observing the effect of their suggested control. Instead, the robot moves on to the next state where it asks for a label.  The human supervisor has to predict how the world will change given a control, instead of operating the robot with a feedback loop. This makes labels much more noisy (Fig. \ref{fig:teaser}(b))).

A second cost is that the data set obtained by getting labels on states that occur due to previous errors corresponds to a more complex policy than standard data set obtained through regular demonstrations. In particular, the supervisor has to show the robot complex recovery maneuvers that do not occur during normal task executing. Imagine, for instance, the task of singulating an object from a set of objects on a table (Fig. \ref{fig:teaser}. Standard demonstrations involve moving forward and from side to side to push obstacles out of the way and recover the desired object. But if the robot makes errors and ends up in a failure state (Fig. \ref{fig:teaser}(c,right)), then more complex maneuvers will be required.  
A potential outcome of this effect is the robot's policy can incur larger learning error and not be able to recover the supervisor's policy. 
 
We explore the value of the extra overhead of RC in particular for emerging classes of highly-expressive deep learning models such as deep decision trees and neural networks. These learners can more accurately learn the supervisor's policy compared to standard learners (like linear models). 

We  added expressiveness can help alleviate model error~\cite{vapnik1992principles}, which means the robot may now recover the expected supervisor's policy with enough data. In this case, the robot will eventually converge to the  supervisor's distribution with HC sampling. Thus,  it could be beneficial to collect data from the distribution it is converging towards rather than its current. 

We explore these ideas in a series of simulated robotic tasks. By varying the expressiveness of the robot's policy, we show on 100 randomly generated grid world environments that performance gap between the two methods diminishes as the expressiveness is increased. We further demonstrate in a point mass control example how RC LfD can prevent convergence by visiting harder to learn areas. 

We then perform a pilot study with 10 students at UC Berkeley, who were asked to use both RC and HC Lfd to train a Zymark robot to singulate an object (or separate it from a pile). We observed a statistically significant gap in the average performance of the policies trained by the two approaches. We found that RC was $40\%$ successful and HC was $60\%$ with the same amount of $60$ trials provided. In our post analysis, we examine how well people could match their controls in retroactive feedback compared to teleoperation. Furthermore, we analyze a held out test set's surrogate loss to explain how well the policy learned can generalize to unseen data. A higher test loss can indicate harder to learn examples were in the dataset. 

We finally present two theoretical contributions, which help analyze our empirical results. First, we show that RC methods can prevent a robot from recovering the supervisor's policy when HC can. We also contribute a new analysis for the error accumulation in HC LfD, which demonstrates for squared Euclidean losses and normally distributed stochastic policies a tighter bound (in the time horizon of the task) than the rate shown by Ross et al.~\cite{ross2010efficient} for HC LfD.

Overall, our results suggest that for highly expressive deep learners, it may be more advantageous to get demonstrations using HC LfD than to ask the human supervisor for retroactive feedback along the current robot's policy.


\section{Related Work}
Below we summarize related work in HC and RC LfD and their theoretical insights. 

\noindent \textbf {\ns}Pormeleau et al. used \ns to train a neural network to drive a car on the highway via demonstrations provided by a supervisor. To reduce the number of examples needed to perform a task, they synthetically generated images of the road and following labels~\cite{pomerleau1989alvinn}.  Schulman et al. used HC LfD for rope tying using kernelized interpolation as the policy class~\cite{schulman2016learning}. 

Ross et al. examined \ns  derived that the error in the worst case from this approach can go quadratic in the time horizon, $T$~\cite{ross2010efficient}. The intuition behind this analysis is that if the distribution induced by the robot's policy is different than the supervisor's, the robot could incur the maximum error. Expressive policies can manage constant $T$ factors by achieving very low training error. We also show that a rate of $T\sqrt{T}$ is achievable for stochastic policies. 

\noindent \textbf{\nc}
\nc has been used in numerous robotic examples. Successful robotic examples of \nc with a supervisor include flying a quadcopter through a forest where the state space is image data taken from an onboard sensor~\cite{ross2013learning}.

 Laskey et al. applied \nc to manipulation tasks such as surgical needle insertion \cite{laskeyshiv} and robotic de-cluttering, where a robot is given image data of a table with a variety of objects on it and must learn to push the obstacle objects aside to grasp a goal \cite{laskeyrobot}. Other successful examples have been teaching a robotic wheelchair to navigate to target positions and teaching a robot to follow verbal instructions to navigate across an office building \cite{kim2013maximum, duvallet2013imitation}. 

Algorithmic extensions to \nc have also been recently made, such as forcing the supervisor to provide controls that are easier for the robot to learn from~\cite{he2012imitation}.  Kim et al. proposed only to query the supervisor in states that the robot is uncertain~\cite{kim2013maximum} . Laskey et al. extended this approach to high-dimensional states~\cite{laskeyshiv}. Laskey et al. examined a hierarchy of supervisors (ranked by quality)  to reduce the burden on an expert demonstrator~\cite{laskeyrobot}. Zhang and Cho trained a classifier to detect when \nc would encounter dangerous states during policy execution~\cite{zhang2016query}.

All these approaches build theoretically upon the online optimization analysis that Ross et al. proposed~\cite{ross2010reduction}. In the online optimization algorithm, a bound for the error in the robot's policy is linear in $T$ for strongly convex losses (i.e.  regularized linear or kernelized regression). Their analysis uses a regret style approach, which says the robot's error is bounded by the worst it could have done in hindsight. Thus, if a robot visits hard to learn states, it could do arbitrarily. We analyze this effect and show how it can prevent RC from converging to the supervisor's policy. 


\noindent \textbf{LfD Interfaces}
Standard techniques for providing demonstrations to a robot are teleoperation, kinesthetic and waypoint specification~\cite{akgun2012keyframe,akgun2012novel}. Kinesthetic teaching is defined as moving the robot body through via a human exerting force on the body. Teleoperation uses an interface such as a joystick or video game controller to control the position of the robot end effector. Waypoint specification has a human specify positions in the space the robot needs to visit. All of these methods are forms of HC sampling because the human guides the robot. 

Previous work has studied these HC sampling techniqes~\cite{akgun2012keyframe,akgun2012novel} relative to each other. However, we look specifically at teleoperation and compare it to RC's form of retroactive feedback. 

\section{Problem Statement and Background}\label{sec:PS}
The goal of this work is to learn a policy that matches that of the supervisor on a specified task that demonstrations are collected on. 

\noindent\textbf{Modeling Choices and Assumptions}  We model the system dynamics as Markovian, stochastic, and stationary. Stationary dynamics occur when, given a state and a control, the probability of the next state does not change over time. 

We model the initial state as sampled from a distribution over the state space.
We assume a known state space and set of controls. We also assume access to a robot or simulator, such that we  can sample from the state sequences induced by a sequence of controls.   Lastly, we assume access to a supervisor who can, given a state, provide a control signal label. We additionally assume the supervisor can be noisy and imperfect. 



\noindent\textbf{Policies and State Densities.}
Following conventions from control theory, we denote by $\mathcal{X}$ the set consisting of observable states for a robot task, consisting, for example, of 
high-dimensional vectors corresponding to images from a camera, or robot joint angles and object poses in the environment.
We furthermore consider a set $\mathcal{U}$ of allowable control inputs for the robot, which can be discrete or
continuous. We model dynamics as Markovian, such that the probability of state $\mathbf{x_{t+1}}\in
\mathcal{X}$ can be determined from the previous state $\mathbf{x}_t\in\mathcal{X}$ and control input $\mathbf{u}_t\in
\mathcal{U}$: 
$$p(\bx_{t+1}|\bu_{t},\bx_{t}, \ldots, \bu_{0}, \bx_{0})=p(\bx_{t+1}|\bu_{t}, \bx_t)$$
We assume a probability density over initial states $p(\bx_0)$. The environment of a task is thus defined as a specific instance of a control and state space, initial state distribution and dynamics. 


%We denote the probability density over the initial state also by $p:\mathcal{X}\to \mathbb{R}$. 

A trajectory $\tau$ is a finite sequence of $T+1$ pairs of states visited and corresponding
control inputs at these states, $\bar{\tau} = ((\mathbf{x}_0,\mathbf{u}_0), ...., (\mathbf{x}_T,\mathbf{u}_T))$, where $\bx_t\in \mathcal{X}$
and $\bu_t\in \mathcal{U}$ for $t\in \{0, \ldots, T\}$ and some $T\in \mathbb{N}$.  



A policy is a measurable function $\pi: \mathcal{X} \to \mathcal{U}$ from states to control inputs. 
We consider a set of policies $\pi_{\theta}:\mathcal{X}\to \mathcal{U}$ parameterized by some $\theta\in \Theta$. Any such policy $\pi_{\theta}$ in an environment with probabilistic initial state density and Markovian dynamics
induces a density on probability measure over the set of  trajectories of length $T+1$: $$p(\tau | \theta)=
p(\bx_0)\prod_{i=0}^{T-1}p(\bx_{t+1}|\bu_t,\bx_t)p(\bu_t|\bx_t,\theta)$$


The term $p(\bu_t|\bx_t,\theta)$ indicates a non-deterministic policy, which can occur due noise in robot execution~\cite{mahler2014learning}. While we do not assume knowledge of the distributions corresponding to: $p(\bx_{t+1}|\bx_t,\bu_t)$, $p(\bx_0)$ or $p(\bx_t|
\theta)$, we assume that we have a stochastic real robot or a simulator such that for any state
$\bx_t$ and control $\bu_t$, we can observe a sample $\bx_{t+1}$ from the density $p(\bx_{t+1}|\bu_t,\bx_t)$. 
Therefore, when 'rolling out' trajectories under a policy
$\pi_{\theta}$, we utilize the robot or a simulator to sample the resulting stochastic trajectories rather than
estimating $p(\bx|\theta)$ itself.


\noindent\textbf{Objective.} The objective of policy learning is to find a policy that minimizes some known reward function $R(\hat{\tau}) = \sum^T_{t=1} r(\bx_t,\bu_t)$ of a trajectory $\hat{\tau}$. The reward $r:\mathcal{X}\times \mathcal{U}\to \mathbb{R}$ is typically user defined and task specific. 
For example in the task of grasping, the reward can be a binary measure of success.

In our problem, we do not have access to the reward function itself. Instead, we only have access to 
a supervisor, $\pi_{\theta^*}$, where $\theta^*$ may not be contained in $\Theta$. A supervisor is chosen that can achieve a desired level of performance on the task. The supervisor provides the robot an initial set
of $N$   demonstration trajectories $\lbrace \tau^1,...,\tau^N \rbrace$. 
which are the result of the supervisor applying this policy. This induces a training data set $\mathcal{D}$ of all state-control input pairs from the demonstrated trajectories.

We are interested in determining what parameter $\theta$ generates the sample demonstrations from the supervisor's policy. 

\noindent \textbf{\ns} \ns frames this  question as maximizing the conditional likelihood of the sample demonstrations conditioned on a given parameter $\theta$. 

$$\underset{\theta}{\mbox{max}} \prod^N_{n=1} p(\bx_{0,n}) \prod^T_{t=1} p(\bx_{t+1,n}|\bx_{t,n},\bu_{t,n})p(\bu_{t,n}|\bx_{t,n},\theta)$$

In solving this optimization it is common to optimize the conditional log-likelihood, which maintains the same solution but breaks up the product terms into sums. 

\begin{equation}\label{eq:m_likeli_obj}
\underset{\theta}{\mbox{max}} \sum^N_{n=1}\sum^T_{t=1}\mbox{log }p(\bu_{t,n}|\bx_{t,n},\theta)
\end{equation}


We note that the dynamics and initial state distributions are dropped in this objective because they are conditionally independent of $\theta$, once the controls are observed. 

 Traditionally maximizing the probability of a given control  from the supervisor has been viewed as minimization of a surrogate loss~\cite{ross2010reduction,ross2010efficient}. We will refer to the function $l : \mathcal{U} \times \mathcal{U} \rightarrow \mathbb{R}$ as the surrogate loss throughout this paper. The surrogate loss can either be an indicator function as in classification or a continuous measure on the sufficient statistics of $p(\bu|\bx,\theta)$.  This rewrites the objective as follows: 

\begin{equation}\label{eq:main_obj}
\theta^N = \underset{\theta}{\mbox{argmin}} \sum^N_{n=1}\sum^T_{t=1} l(\bu_{n,t}, \pi_{\theta} (\bx_{n,t})).
\end{equation}


\noindent \textbf{\nc} \mlnote{need to shorten this} If $\theta* \in \Theta$, then as N grows, $
\theta^N$ will approach an objective value (i.e. loss) of equation (2) equal to that evaluated under $\theta^*$ (note, this does not necessarily imply $\theta^N \rightarrow \theta^*$ unless  Eq . \ref{eq.main_obj} is strongly convex with respect to theta). However, the states that are being used to train $\theta^N$ are provided by $\theta^*$. Thus, when the policy is rolled out under $\theta^N$ (and $\theta^N \neq \theta^*$ exactly) then it is possible that new states will be visited that would never have been visited by $\theta^*$. To account for this, prior work has proposed an iterative solution ~\cite{ross2010reduction} that attempts to solve this problem by aggregating data on the distribution induced by the current robot's policy.

Instead of  minimizing the surrogate loss in Eq. \ref{eq:main_obj},  RC LfD, or DAgger and its variants~\cite{ross2010reduction,laskeyshiv,he2012imitation}, attempts to find the distribution the final policy will converge to. Thus, reducing surrogate loss in places the robot is likely to visit.
We will specifically describe DAgger~\cite{ross2010reduction}, which attempts this by iterating two steps: 1)
computing the policy parameter $\theta$ using the training data $\mathcal{D}$ thus far, and 2) by executing the policy
induced by the current $\theta$, and asking for labels for the encountered states. 
 
\subsubsection{Step 1}
The first step of any iteration $k$ is to compute a $\theta_k$ that minimizes surrogate loss on the current dataset $\mathcal{D}_k=\{(x_i,u_i)|i\in\{1,\ldots,N\}\}$ of demonstrated state-control pairs (initially just the set $\mathcal{D}$ of initial trajectory demonstrations):

 \vspace{-1ex}
\begin{align}\label{eq:super_objj}
\theta_{k} = \underset{\theta}{\argmin} \: \sum_{i=1}^{N} \sum_{t=1}^T  l(\pi_{\theta}(\bx_{i,t}),\bu_{i,t}).
\end{align}

This can be observed as minimizing the empirical risk on the aggregate dataset of all examples seen so far~\cite{scholkopf2002learning}.  Note that equal weight is given to each example regardless of how likely they are under the current policy.
 

 \subsubsection{Step 2}
The second step at iteration $k$, DAgger rolls out the current policy, $\pi_{\theta_{k}}$, to sample states that are likely under $p(\tau|\theta_{k})$.  For every state visited, DAgger requests the supervisor to provide the appropriate control/label. Formally, for a given sampled trajectory  $\tau = (\bx_0,\bu_0,...,\bx_T,\bu_T )$, the supervisor provides labels $\tilde{\bu}_t$, where $\tilde{\bu}_t \sim \tilde{\pi}(\bx_t) + \epsilon$, where $\epsilon$ is a  zero mean noise term, for $t\in \{0, \ldots, T\}$.
The states and labeled controls are then aggregated into the next data set of demonstrations $\mathcal{D}_{k+1}$:
$$D_{k+1}=\mathcal{D}_k \cup \{(\bx_t,\tilde{\bu_t})|t\in\{0,\ldots,T\}\} $$

Steps 1 and 2 are repeated for $K$ iterations or until the robot has achieved sufficient performance on the
task\footnote{In the original DAgger the policy rollout was stochastically mixed with the supervisor, thus with
    probability $\beta$ it would either take the supervisor's action or the robots. The use of this stochastically mix
    policy was for theoretical analysis and in practice, it was recommended to set $\beta = 0$ (i.e. RC only) to avoid biasing the
sampling~\cite{NIPS2014_5421,ross2010reduction}}.


 


\section{Experiments}

\begin{figure*}
\includegraphics{f_figs/var_grid.eps}
\caption{
    \footnotesize
Shown above is the normalized performance with respect to the expected supervisor, where $1.0$ indicates matching the optimal supervisor's performance. The plots are averaged over 100 randomly generated 2D gridworld environments,  where the robot is taught to avoid penalty states and reach a goal state. Both RC and HC are given the same number of samples. A) Examines when the robot's policy class has low expressiveness (i.e. Linear SVM), which results in RC  leading to better performance. Condition B examines a more expressive robot policy class (i.e. Decision Trees) that contains the expected supervisor,and demonstrates negligible difference between HC and RC.  to represent the supervisor.  Finally, Condition C examines when the supervisor has noise added to the controls labels, this leads to more data being needed to converge to the expected supervisor. HC and RC perform the same in this situation.  }
\vspace*{-20pt}
\label{fig:var}
\end{figure*}

We provide experiments in a  Grid World environment, which allows for us to vary the robot's policy class over a large number of randomly generated environments. Then we examine a linear dynamical system, which is used to show a limitation the ability of RC LfD to converge. Finally, we perform human trials on 10 participants, who try to teach a robot how to singulate, or separate an object from a pile. We used DAgger as the example of an RC algorithm in these experiments. 

\subsection{Varying Function Class}\label{sec:gdw}
In this experiment, we hypothesize that the performance gap of RC LfD diminishes as the robot's policy becomes more expressive. In a simulated Grid World, we have a robot that is trying to reach a goal state, at which it receives $+10$ reward. The robot receives $-10$ reward if it touches a penalty state. The robot also receives a $-0.02$ penalty for every blank state. The robot has a state space of $(x,y)$ coordinates and a set of actions consisting of $\lbrace$LEFT, RIGHT, FORWARD, BACKWARD, NONE$\rbrace$, where NONE indicates staying at the current stop. The grid size for the environment was $30 \times 30$. $20\%$ of randomly drawn states are marked as a penalty, while only one is a goal state. For the transition dynamics, $p(\bx_{t+1}|\bx_{t},\bu_t)$, the robot goes to an adjacent state that does not correspond to the intended control with probability $0.2$.  The time horizon for the policy is $T=30$. 

We use Value Iteration to compute an optimal supervisor. The supervisor must learn to be robust to the noise in the dynamics, reach the goal state and then stay there. In all settings we provided the RC LfD with one demonstration from HC sampling and then roll out its policy. This initial demonstration set from HC sampling is common in methods like DAgger~\cite{rossreduction2010}.

Each plot shows the normalized performance where $1.0$ represents the expected performance of the optimal supervisor. We run all trials over 100 randomly generated environments, which contain $270$ randomly selected penalty states and $1$ goal state. 


\noindent \textbf{Low Expressiveness} In Fig. \ref{fig:var} A, we show the case when the robot's policy class does not contain the optimal supervisor's policy, we used a Linear SVM for the policy class representation, which is commonly used in ~\cite{ross2010efficient,ross2010reduction,ross2013learning} . As illustrated, the RC sampling approach does better than HC sampling, which is consistent with prior literature~\cite{ross2010efficient,ross2010reduction}.

This outcome suggests that when the robot is not able to learn the supervisor's policy, RC LfD has an advantage. However, it is important to note that neither methods are able to converge to the supervisor's performance. 

\noindent \textbf{High Expressiveness}
We next consider the situation where the robot's policy class is more expressive. We used decision trees with a depth of $100$ to obtain a complex enough function class to achieve this. 

As shown in Fig. \ref{fig:var}, RC LfD and HC LfD both converge to the supervisor, but at the same rate. Thus, suggesting that advantage of using RC techniques diminishes once the robot's policy is more expressive. We note that because the robot's policy class can represent the supervisor, both RC and HC methods converge to the supervisor's performance.


\noindent \textbf{Noisy supervisor}
We finally observe the effects of noise on the supervisor. Here we consider the case where noise is applied to observed label, thus the robot receive control labels that are $\bu = \pi_{\theta}(\bx) + \epsilon$,  where epsilon is an i.i.d distribution that selects a random control with probability $0.3$.

We use the  decision tree  of depth $100$ which due to its large function class is more susceptible to the noisy supervisor. We then compare the performance of HC vs. RC LfD. As shown, both method are able to converge to the expected supervisor's normalized performance because they can average out the noise in the labels with enough data. Though it  takes more data both methods (HC and RC) converge at a similar rate to the true expected supervisor. 




\subsection{Algorithmic Convergence }
In this experiment, we evaluate when HC methods converge to the superivsor's performance and RC does not. We consider the example where a robot needs to learn to get to a location in a 2D continuous world domain. The robot is represented as a point mass with linear dynamics and the supervisor is computed using the infinite time horizon LQG, which results in a linear policy. 

The environment contains two sets of dynamics (1 and 2): 

$$\bx_{t+1} = A\bx_{t+1}+B_1\bu_t+w$$
$$\bx_{t+1} = A\bx_{t+1}+B_2\bu_t + w$$

where $w\sim \mathcal{N}(0,0.1 I)$. The state space, $\bx$,for the robot is $(x,y,v_x,v_y)$ or the coordinates and the velocity. The controls, $\bu$, for the robot is $(f_x,f_y)$, which are forces in the coordinate direction. The matrix $A$ is a $4\times4$ identity matrix plus the necesary two values to update the $x,y$ state by the velocity with a timestep of $1$. $B_1$ and $B_2$, correspond to a $4\times 2$ matrix that updates only the velocity for each axis independently. This update corresponds to $\mathbf{v} = \frac{1}{m} \mathbf{f}$, where $m$ is the mass of the robot. 


 The dynamics for region 1 correspond to the point robot having a mass of $m=1$ and in region 2 the point robot has a larger mass of $m=4$. A target goal state lies at the origin $[0,0]$ and the robot starts out at the point $[-15,-10]$ with a velocity of zero. The boundary for region 1 and 2 lies at $x=12$ and $y=12$, where greater than indicates region 2. An illustration is shown in Fig. \ref{fig:p_mass}. The time horizion for the task is $T=35$. 

The supervisor is a switching linear system, where each linear model is computed via the infinite horizon LQG for the specified dynamics. The robot gets feedback from the supervisor computed for the region that it is currently in.  The robot's policy, $\pi_{\theta}$ is represented as a linear policy which is found via least squares. 

We run HC and RC LfD in this setting and plot the performance in Fig. \ref{fig:p_mass} averaged over $200$ trials.  As shown, HC LfD is able to converge to the true supervisor's policy however the RC approach  forces the robot to enter region 2 of the work space and subsequently try to learn the two different supervisors. Thus, preventing it from converging. 

\begin{figure}
\centering
\includegraphics{f_figs/p_mass.eps}
\caption{
    \footnotesize
Left: A 2D workspace where a point mass robot is taught to go to the green circle starting from the blue circle. The world is divided into to two quadrants 1 and 2, in 2 the point mass has four times as much mass. The supervisor is computed via infinite horizon LQG for each region, which results in two different linear matrices in region 1 and 2. Right: Illustrates how having a linear robot policy class can cause RC LfD to fail  to converge due to it collecting data from region 2, however HC converges to the true supervisor performance. }
\vspace*{-1pt}
\label{fig:p_mass}
\end{figure}

\subsection{Human Study for Planar Singulation}
We perform a human study on a real robot, where participants teach the robot to perform a singulation task (i.e. separate an object from its neighbors) see Fig. \ref{fig:izzy_sing}. A successful singulation means at least one object has its center located 10 cm or more from all other object centers. Our hypothesis is twofold: 1) RC sampling will force the robot to try and learn more complex behavior. 2) Participants will struggle with providing retroactive feedback. 

The robot has a two-dimensional control space, $\mathcal{U}$, of base rotation and arm extension. The state space of the environment, $\mathcal{X}$, is captured by an overhead Logitech C270 camera, which is positioned to capture the workspace that contains all cluttered objects and the robot arm.  The objects are red extruded polygons. We consider objects made of Medium Density Fiberboard with an average 4" diameter and 3" in height.  We use the current image of the environment as the state representation, which captures positional information. The policy is a deep neural network, the same as in~\cite{laskeyrobot}. The network is trained using TensorFlow~\cite{tensorflow2015-whitepaper} on a Telsa K40 GPU. 

\begin{figure}
\centering
\includegraphics{f_figs/singulation.eps}
\caption{
    \footnotesize
Left: An example initial state the robot observes. The initial state can vary the relative position of the objects and pose of the pile. Right: A human is asked to singulate the object, which is to have the robot learn to push one object away from its neighbors. A successful singulation means at least one object has its center located 10 cm or more from all other object centers.   }

\label{fig:izzy_sing}
\end{figure}


\begin{figure}
\centering
\includegraphics{f_figs/labeling.eps}
\caption{
    \footnotesize
Two ways to provide feedback to the robot. a)In HC sampling, the human teleoperates the robot and performs the desired task. For the singulation task, the human supervisor used an Xbox Controller. b) In RC sampling, the human observes a video of the robot's policy executing and applies retroactive feedback for what the robot should have done. In the image shown, the person is telling the robot to go backward towards the cluster.  }

\label{fig:labeling}
\end{figure}
The robot is moved via positional control implemented with PID. Similar to \cite{laskeyshiv}, the controls, $\mathcal{U}$,  are bounded changes in rotation and translation. The control signals for each degree of freedom are continuous values with the following ranges: base rotation, $[-1.5^\circ,1.5^\circ]$, arm extension $[-1,1]$. The units are degrees for base rotation and centimeters for arm extension. 

During training and testing the initial state distribution $p(\bx_0)$ consists of sampling the 2D pose of the object cluster from a multivariate Gaussian \mlnote{get numbers}. Then the relative position of the 4 objects from a uniform distribution.  A virtual overlay over the webcam was used to help a human operator place the objects in their correct pose. 

The robot can be trained by either RC LfD or HC LfD.  In HC LfD, the subject is asked to provide 60 demonstrations to the robot using an Xbox Controller, as shown in Fig. \ref{fig:labeling}a. The user is first asked to provide 20 demonstrations via the Xbox Controller and then provides retroactive feedback for $K=2$ iterations of 20 demonstrations each. Retro-active feedback is provided through a labeling interface (similar to Laskey et al. \cite{laskeyrobot}), shown in Fig \ref{fig:labeling}b. In this interface, we show a slowed down video (by 2x) of the robot's rollout. The supervisor uses a mouse to provide feedback in the form of translation and rotation. We provide a virtual overlay for to human supervisors to see the magnitude of their given control. 

10 human subjects, who are UC Berkeley students, were selected for the experiment. The subjects were familiar with robotics, but not the learning algorithms behind the techniques. They first watched a trained robot perform the task successfully.  They then practiced providing feedback through RC sampling for 5 demonstrations and HC for 5 demonstrations. Next, each subject performs the first 20 demonstrations via HC sampling.  Then using counter-balancing, we select whether they will do HC or RC sampling for the next 40 demonstrations.  They finally perform the other sampling method for 40 demonstrations. The experiment on average took 2 hours per person. 

In Fig. \ref{fig:izzy_rw} , we show the average performance of the policies trained with RC and HC LfD. Each policy is evaluated on a holdout set of 30 initial states sampled from the same distribution as training. As shown, the policies learned with RC exhibit statistically significant worse performance than those with HC LfD (21\%). Thus, suggesting that RC can perform worse than HC LfD on a task with actual human supervision. 


\noindent \textbf{Post Analysis}
To better understand why RC performed worst than HC LfD, we first wondered if the RC method tried to teach the robot more complex behavior. In Fig. \ref{fig:teaser}c, we show trajectories collected on a single state during the study. As illustrated HC sampling is concentrated around a path needed to singulate the bottom right object. However, RC sampling places the robot in a wide variety of states, some of which would require the robot learning how to move backward to recover. 

To better analyze this, we examined the surrogate loss on a test set of 10 randomly selected trajectories from each supervisor's dataset.  As shown in Table 1, we observed the average test error over the policies trained with 60 demonstrations in both degrees of freedom (i.e. translation and rotation) is significantly higher.  The difference in test loss does not imply the data is necessarily more complex.  It does indicate though that the RC policies on average had a harder time generalizing to unseen labels in their aggregate dataset. 




\begin{table}[t]
\centering
\begin{tabular}{ R{1.75cm}||R{2.5cm}| R{2.5cm}}
 %\hline
 %\multicolumn{4}{|c|}{Sensitivity Analysis for Convergence to Best Grasp for Thompson sampling} \\
 %\hline 
 & \multicolumn{2}{c}{Surrogate Loss on Test Set} \\
 \hline
\specialcell{\bf Algorithm\\ \bf Type} & \specialcell{\bf Translation \bf (mm)} & \specialcell{\bf Rotation \\ \bf (rad)} \\
 \hline
HC LfD & $2.1\pm 0.2$ & $0.009 \pm 0.001$ \\
RC LfD & $3.4 \pm 1.0$    & $0.014 \pm 0.003$ \\
 %\hline
\end{tabular}
   \caption { \footnotesize  Shown above is the average surrogate loss on a held out set of 10 demonstrations from the the total 60 demonstrations collected for each 10 demonstrators. The confidence intervals are standard error on the mean, which indicate RC LfD obtains a statistically significant higher surrogate loss in both degrees of freedom, forward and rotation. 
   }
		\tablabel{opt-p-comparison}
\vspace*{-10pt}
\end{table}



\begin{figure}
\centering
\includegraphics{f_figs/cor_data.eps}
\caption{
    \footnotesize
    Results from the post analysis examining how well retroactive feedback matched teleoperation. The scatter plot shows the normalized angle of the control applied for both HC (teleoperation) and RC (retroactive).  The large dispersion in the graph indicates that the five participants had a difficult time matching their retroactive and teleoperated controls. Two example trajectories are also shown.  The black line indicates the path from teleoperation and the teal line is the direction and scaled magnitude of the feedback given. If they matched perfectly, the teal line would be tangent to the path. 
}

\label{fig:izzy_traj}
\end{figure}




We next hypothesized that the participants could have had trouble providing retroactive feedback. To test this, we asked 5 of the participants to provide 5 demonstrations via teleoperation. Then we asked them to try and match their controls via the RC labeling interface. 

We measured the correlation on between the controls applied via retroactive feedback and teleoperation. When calculated over all the participants and trajectories the correlation, or Pearson Coefficient, in rotation and translation, were 0.60 and 0.22, respectively. A smaller correlation coefficient suggests that it is harder for people to match their control in teleoperation. 

In Fig. \ref{}, we plot the angle of the controls applied for RC and HC in the post analysis. We also show two trajectories that a participant teleoperated with their retroactive labels overlayed. The disagreement between the teleoperated and retroactive can be of high magnitude. Overall this post analysis suggests that RC's form of retroactive feedback can lose the intent of the human supervisor. 


\begin{figure}
\centering
\includegraphics{f_figs/izzy_reward.eps}
\caption{
    \footnotesize
Shown is averaged success at the singulation task over the 10 human subjects. Each policy is evaulated 30 times on the a held out set of test configurations. The first 20 rollouts are from the supervisor rolling out there policy and the next 40 are collected via retro-active feedback for adaptive and tele-operated demonstrations for passive. HC LfD shows a 20$\%$ improvement in success at the end. }

\label{fig:izzy_rw}
\end{figure}


\section{Theoretical Analysis}
To better understand the empirical results from above, we contribute new theoretical analysis on RC and HC LfD. In this section we will first show how when HC is guaranteed to be consistent, RC is not.  Then we present a new analysis of the accumulation of error for  \ns. In this section are interested in the total loss along a trajectory with respect to the supervisor's policy $\pi_{\theta^*}$, which is defined as $J(\theta) = \sum^T_{t=1} l(\pi_{\theta}(\bx_{t}),\pi_{\theta^*}(\bx_{t}))$. 

\subsection{Algorithm Consistency}
\mlnote{Kevin is working on a better more general version of this section, will update once he gets done}
A policy produced by an LfD algorithm is {\it consistent} if it can match the loss of the supervisor on the supervisor's own distribution given infinite samples:
$$\underset{N \rightarrow \infty}{\text{lim }} E_{p(\tau|\theta^N)}J(\theta^N)  = E_{p(\tau|\theta^*)}J(\theta^*) $$
\noindent where $\theta^N$ is the policy produced by running the LfD algorithm on $N$ datapoints.
We note that this condition does not imply {\it convergence} to the supervisor's policy: $\underset{N \rightarrow \infty}{\text{lim }} \theta^N = \theta^*$.

We show that there exist environments such that \ns is consistent but \nc is not.
Let $\theta_{h}^M$ be the result of running \ns on $M$ trajectory samples from the supervisor.
Let $\theta_{r}^N$ be the result of running \nc for $N$ iterations.

\begin{figure}
\centering
\includegraphics{f_figs/counter_exmp.eps}
\caption{
    \footnotesize
A Directed Acyclic Graph, where a robot is being taught by a supervisor to descend down and achieve maximum cumulative reward, which is shown via the numbers on each branch. Each node has the action the supervisor would select, either left or right, $\lbrace L, R \rbrace$. The HC method converges to the Orange path, which is optimal. While the RC method converges to the Teal path, because it tries to learn on examples from that side of the tree.}
\vspace*{-20pt}
\label{fig:c_ex}
\end{figure}

\begin{theorem}
There exist an environment and policy class $\Theta$ such that:
\vspace{-2ex}
\begin{align*}
	\underset{N \rightarrow \infty}{\text{lim }} E_{p(\tau \mid \theta_{r}^N) } J(\theta_{r}^N) &> E_{p(\tau \mid \theta^*)} J(\theta^*) \\
	\underset{M \rightarrow \infty}{\text{lim }} E_{p(\tau \mid \theta_{h}^M) } J(\theta_{h}^M) &= E_{p(\tau \mid \theta^*)} J(\theta^*)
\end{align*} 
\end{theorem}

\begin{proof}
Consider an environment with a deterministic initial state $\bx_0$ at the root of a binary tree of depth 3 with deterministic dynamics and rewards illustrated in Fig. \ref{fig:c_ex}.
Let $T = 3$ and let $\ell$ be the 0-1 surrogate loss.
The optimal policy is to choose to go left at $\bx_0$ and every branch to the left of the tree, and to right at every branch on the right of the tree.
This incurs reward $E_{p(\tau \mid \theta^*)} R(\tau) = 30$ and expected surrogate loss $E_{p(\tau \mid \theta^*)} J(\theta^*) = 0$.

Let the policy class for RC and HC LfD be the set of policies $\Theta = \{L, R\}$.
Let $\mD_K$ be a set of $K * T$ states generated by the algorithm, and let $a = \sum_{x \in \mD_K} \mathbf{1}(\pi^*(x) = R)$ and $b = \sum_{x \in \mD_K} \mathbf{1}(\pi^*(x) = L)$ count the number of states in which the supervisor chose right and left, respectively, where $\mathbf{1}$ is the indicator function. 
Then the policy produced by either algorithm is $\pi_{\hat{\theta}}$ for all states $x$, where
\vspace{-2ex}
\begin{align*}
	\theta^K = \underset{\theta \in \Theta}{\text{argmin }} \sum_{k=0}^K \sum_{t=0}^T \ell(\pi_{\theta}(x_{k,t}), \pi_{\theta^*}(x_{k,t})) = \left\{ \begin{array}{cc} R & a \geq b \\ L & a < b \end{array} \right.
\end{align*}
\noindent In other words, the optimal policy is to choose the action that the supervisor chooses most frequently in the dataset.

For \ns, the dataset $\mD_K$ consists of $K * T$ left labels because the dynamics and initial state are deterministic.
Therefore for any $M > 0$  HC LfD chooses $\theta_{h}^M = L$ which has surrogate loss $E_{p(\tau \mid \theta^M)} J(\theta_{h}^M) = 0$.

Let \nc be initialized with $\theta_{r}^0 = R$ and without loss of generality assume RC collects one trajectory per demonstration and is run with $\beta=0$~\cite{ross2010reduction}.
Then on iteration $N=1$, \nc goes to the right three times and collects supervisor labels $\{L, R, R\}$, leading to policy $\theta_{d}^1 = R$.
Furthermore, if $\theta_{d}^N = R$ then the next iteration collects $\{L, R, R\}$ and the dataset $\mD_{K+1}$ maintains two-thirds right labels, so $\theta_{d}^{N+1} = R$.

Therefore by induction $\theta_{d}^{N} = R$ for all $N > 0$, which incurs surrogate loss $E_{p(\tau \mid \theta_{p}^N)} J(\theta_{p}^N) = (1 / 3) T > E_{p(\tau \mid \theta^M)} J(\theta^M)$.
We also note that the reward is also suboptimal: $E_{p(\tau \mid \theta^N)} R(\tau) = 20$.

\end{proof}


In online optimization a learner plays a game where at each iteration, it chooses a policy and receives a loss from an adversary.  In the LfD setting, the learner is the robot's policy and the adversary would be the loss on the distribution of states induced by the policy $E_{p(\tau|\theta)} J(\theta)$~\cite{shalev2011online}.

RC LfD in this context is known as a Follow the Leader (FTL) algorithm. In FTL, the best policy is chosen on all previous seen losses, or the aggregate dataset in the LfD context. Their analysis shows that under the condition when the surrogate loss, $l$, is strongly convex with respect to $\theta$, their policy has a regret that converges to the best the policy could have done on all previous seen losses $\underset{\theta}{\min} \sum_{k=1}^K E_{p(\tau|\hat{\theta}_k)}J(\hat{\theta}_k)$. 

%A bound in regret though though does not necessarily imply the robot will be able to generalize to the unseen data nor match the supervisor. It instead says the robot's error is bounded by the best it could have done in hindsight, which may be arbitrarily bad as our tree example suggests. 

\subsection{Bound on Error for HC Lfd}
In the HC LfD setting a robot is trained on the states visited by the supervisor. However, at run time the robot may encounter a different distribution of states due to not perfectly matching the supervisor's policy. Ross et al. showed that given a time horizon, $T$, the worst case error scales quadratically (i.e. $O(T^2E_{p(\bx|\theta^*)} l(\theta^N)$) when executing the robot's policy~\cite{ross2010efficient}. Note according to the notation of Ross et al., $TE_{p(\bx|\theta^*)} l(\theta^N) = E_{p(\tau|\theta^*)} J(\theta^N)$. We present a new analysis for a class of stochastic policies and  defined below that shows a rate of $O(T\sqrt{TE_{p(\bx|\theta^*)} l(\theta^N)})$.

\mlnote{Talked to Kevin and will be moving all below to a supplement. We feel it isn't a key contribution and shouldn't prioritize this much space}
 Define the surrogate loss as the squared euclidean norm, or $l(\pi_{\theta}(\bx),\pi_{\theta^*}(\bx)) = ||\pi_{\theta}(\bx_{i,t}) - \pi_{\theta^*}(\bx_{i,t})||_2$.We assume that the controls are be bounded and thus can be normalized such that the $l \in [0,1]$.  We are interested in the situation where the supervisor and robot policy are stochastic with a Normal Distribution (i.e. $p(\bu|\pi_{\theta}(\bx)) = \mathcal{N}(\pi_\theta(\bx),\sigma I)$. The interest in stochastic policies instead of deterministic is two-fold: 1) human supervisor can be noisy in nature and 2) due to noise in robot execution, such as cable coupling, the intended and actual control may differ in a stochastic way~\cite{mahler2014learning}. \mlnote{need to clarify what the generalization is} We show a generalization of this bound to any distribution in the exponential family in the supplement material.  Note Ross et al.~\cite{ross2010reduction} used a deterministic policy in their analysis, which may prohibit a direct comparison of rates.

\begin{theorem}
Given a policy $\pi_{\theta^N}$, the following is true 
$$E_{p(\tau|\theta^n)} J(\theta^N) \leq T\sqrt{\frac{1}{4\sigma}E_{p(\tau|\theta^*)} J(\theta^N)}+E_{p(\tau|\theta^*)} J(\theta^N)$$
\end{theorem}
\begin{proof}
For convenience we will write $E_{p(\tau|\theta)} = E_{\theta}$ and $l(\theta,\bx) = l(\theta)$. The proof follows by first deriving an upper bound on the worst case difference between the two quantities $E_{\theta^N} J(\theta^N) - E_{\theta^*} J(\theta^N) $. Then we leverage the intuition that if one is minimizing $E_{\theta^*} J(\theta^N) $, they are also decreasing the distance between the robot and supervisor's distributions. 

\begin{align}
&E_{\theta^N} J(\theta^N) - E_{\theta^*} J(\theta^N) \\
&= T(\frac{1}{T}E_{\theta^N} J(\theta^N) -\frac{1}{T}E_{\theta^*} J(\theta^N)\\
&\leq  T| | p(\tau|\theta^N) - p(\tau|\theta^*)||_{TV}\\
&\leq T\sqrt{\frac{1}{2} D_{KL}(p(\tau|\theta^*),p(\tau|\theta^N))}
\end{align}

 Line 6 leverages the fact that the worst case loss is bounded by $1$ and the definition of Total Variational distance. Line 7 uses Pinsker's inequality~\cite{verdu2014total}.


\begin{align}
&= T\sqrt{\frac{1}{2} E_{p(\theta^*)} \mbox{log} \frac{p(\tau|\theta^*)}{p(\tau|\theta^N)}}\\
&= T\sqrt{\frac{1}{2} E_{p(\theta^*)} \sum^T_{t=1}\mbox{log} \frac{p(\bu_t|\bx_t,\theta^*)}{p(\bu_t|\bx_t,\theta^N)}}\\
&= T\sqrt{\frac{1}{4\sigma} E_{p(\theta^*)} \sum^T_{t=1} ||\bu_t- \pi_{\theta^N}(\bx_t)||_2^2 - ||\bu_t- \pi_{\theta^*}(\bx_t)||_2^2}\\
&\leq T\sqrt{\frac{1}{4\sigma} E_{p(\theta^*)} \sum^T_{t=1}  ||\pi_{\theta^*}(\bx_t) - \pi_{\theta^N}(\bx_t)||_2^2}\\
&= T\sqrt{\frac{1}{4\sigma} E_{p(\theta^*)} J(\theta^N)}\\
&= T\sqrt{T \frac{1}{4\sigma} E_{p(\bx|\theta^*)} l(\theta^N)}
\end{align}

Line 8,9 and 10 apply the definition of the KL-divergence, the markov chain and the normal distribution over $p(\bu_t|\bx_t,\theta)$. Line 11 applies the triangle inequality to upperbound by the defined surrogate loss. Line 12 applies the assumed definition of $J(\theta)$. Line 13 uses the notation of Ross et al.. 

The intuition behind these steps is that difference between  the two distribution can be controlled via the surrogate loss on the expected supervisor. Thus, illustrating the closer the robot's policy matches the supervisor's policy on the supervisor's distribution, the smaller the total variational difference between the resulting two distributions will be. 
\end{proof}
  
The above analysis demonstrates how the constant $T$ affects the bound in error. However, it is important to note that this is not the only variable that plays a role in performance. The size of the function class and number of demonstrations needed are are also important in determining how large $E_{p(\bx|\theta^*)} l(\theta^N)$ is. 

Understanding how much data is needed to learn a function, is a well studied problem known as sample complexity analysis~\cite{anthony2009neural,bartlett2002rademacher,kakade2009complexity}. In this literature they use different metrics to describe the complexity of a function class and show rates on which a given function class would converge to the best in the set. We refer the reader to \cite{vapnik2013nature}, for a review of such topics. 


\section{Conclusion and Future Work}
\mlnote{need to update tomorrow}
We thus conclude our analysis on the trade-offs between passive and adaptive Lfd approaches. We demonstrated that if a robot's function class contains the expected supervisor's policy, then the  performance difference between adaptive algorithms, such as DAgger,  and passive Lfd techniques can become negligible.  Furthermore, we also provided examples where adaptivity fails to converge despite being able to represent the supervisor. Thus illustrating a short-coming in the regret style analysis for DAgger. 

Our final experiment in having humans teach a robot to singulate objects suggest that adaptivity can lead to worse performance. Our post analysis seems to suggest that humans had a harder time providing retro-active feedback via a labeling interface. While, this may be overcome through a better labeling interface, an inherent problem persists in the inability of a human to observe how well their controls would have performed. 

Finally, we offered a new theoretical  way to analysis passive Lfd and demonstrate that the depedence on the time horizon constant, $T$, can be $O(T\sqrt{T})$ and not $O(T^2)$, a result that does not need the assumption of strongly convexity.  For convex loses, we are able to offer data-dependent sample complexity bounds that illustrate how the function class of the learning algorithm and number of demonstrations effect this bound on error. 

The data-dependent results demonstrate that in the worst-case a heavy tail could exits in terms of the number of demonstrations needed (i.e. a large number of demonstrations could be need). Thus, presenting a challenge in obtaining an industrial level of reliability in unstructured domains. However, in future work we are interested in examining ways around this limitation. Below are two possible ways:

\noindent \textbf{Active Learning} Uniform sampling from the initial state distribution $p(\bx_0)$, may not be an optimal approach to learning the supervisor's policy. In active learning, an algorithm would try to select more informative initial states for the robot to learn from. An example of this is the field of optimal design, where measurements are collected in a way to reduce variance in the label and lead to more robust data analysis. Extending this to LfD would be an interesting opportunity. 

\noindent \textbf{Synthetic Data Generation} Another approach to reduce data-dependence is to synthetically grow your dataset. For example, consider a robot learning how to grasp an isolated object  with image data as the state space. One could take the given image state and translate both the object and grasp label to create more demonstrations. A similar approach has been used recently in self-driving cars. However, a formal approach to checking when synthetic data generation is possible would be an exciting new direction. 

\bibliographystyle{IEEEtranS}
\bibliography{references}


\end{document}
Contact GitHub API Training Shop Blog About
© 2016 GitHub, Inc. Terms Privacy Security Status Help
